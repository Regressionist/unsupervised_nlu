{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank, ptb\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/rj1408/ptb_wsj_pos/train.p\",\"rb\") as f:\n",
    "    traindict = pkl.load(f)\n",
    "with open(\"/data/rj1408/ptb_wsj_pos/val.p\",\"rb\") as f:\n",
    "    valdict = pkl.load(f)\n",
    "with open(\"/data/rj1408//ptb_wsj_pos/test.p\",\"rb\") as f:\n",
    "    testdict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag analysis\n",
    "with open('tagset.txt') as f:\n",
    "    alltags = f.read()\n",
    "\n",
    "alltags = list(map(lambda strline: strline.split('\\t')[1], alltags.split('\\n')))\n",
    "alltags = set(alltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = traindict['tagged_words']\n",
    "tag_cntr = {}\n",
    "for tup in lis:\n",
    "    if tup[1] in alltags:\n",
    "        if tup[1] not in tag_cntr:\n",
    "            tag_cntr[tup[1]] = Counter()\n",
    "        tag_cntr[tup[1]][tup[0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NNP', 11127),\n",
       " ('CD', 4597),\n",
       " ('NNS', 4872),\n",
       " ('JJ', 7958),\n",
       " ('MD', 27),\n",
       " ('VB', 2056),\n",
       " ('DT', 48),\n",
       " ('NN', 9149),\n",
       " ('IN', 187),\n",
       " ('VBZ', 1123),\n",
       " ('VBG', 1987),\n",
       " ('CC', 33),\n",
       " ('VBD', 1560),\n",
       " ('VBN', 2089),\n",
       " ('RB', 1283),\n",
       " ('TO', 4),\n",
       " ('PRP', 46),\n",
       " ('RBR', 51),\n",
       " ('WDT', 12),\n",
       " ('VBP', 975),\n",
       " ('RP', 35),\n",
       " ('PRP$', 15),\n",
       " ('JJS', 103),\n",
       " ('POS', 3),\n",
       " ('EX', 2),\n",
       " ('WP', 8),\n",
       " ('JJR', 151),\n",
       " ('WRB', 14),\n",
       " ('NNPS', 562),\n",
       " ('WP$', 1),\n",
       " ('PDT', 13),\n",
       " ('RBS', 8),\n",
       " ('FW', 101),\n",
       " ('UH', 39),\n",
       " ('SYM', 15),\n",
       " ('LS', 18)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocab size for each tag\n",
    "[(tup[0], len(tup[1])) for tup in tag_cntr.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_cntr = Counter([tup[0] for tup in  lis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43813"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_cntr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9921243360632146"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sent len analysis\n",
    "sentlens = Counter([len(sent) for sent in traindict['tagged_sents']]).most_common()\n",
    "\n",
    "num = sum([tup[1] for tup in sentlens if tup[0] <= 60])\n",
    "den = len(traindict['tagged_sents'])\n",
    "num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPercentile(voc_cntr, topk):\n",
    "    totalsize = sum([tup[1] for tup in voc_cntr.items()])\n",
    "    topksize = sum([tup[1] for tup in voc_cntr.most_common(topk)])\n",
    "    return topksize/totalsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9592498465492572"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPercentile(vocab_cntr, 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  tensor([[0.1106, 0.5225],\n",
      "        [0.4218, 0.6292],\n",
      "        [0.9089, 0.1585],\n",
      "        [0.1656, 0.9648],\n",
      "        [0.7582, 0.1830],\n",
      "        [0.9796, 0.1543],\n",
      "        [0.9654, 0.6308],\n",
      "        [0.0668, 0.3580],\n",
      "        [0.0588, 0.0741],\n",
      "        [0.7088, 0.0137]], requires_grad=True)\n",
      "gtruth:  tensor([ 1,  0,  1,  0,  0,  1,  1, -1, -1, -1])\n",
      "tensor([0.5082, 0.8022, 1.1371, 1.1705, 0.4464, 1.1886, 0.8744, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<NllLossBackward>)\n",
      "tensor([[0.5082],\n",
      "        [0.8022],\n",
      "        [1.1371],\n",
      "        [1.1705],\n",
      "        [0.4464],\n",
      "        [1.1886],\n",
      "        [0.8744]], grad_fn=<MulBackward0>)\n",
      "tensor([0.5082, 0.8022, 1.1371, 1.1705, 0.4464, 1.1886, 0.8744],\n",
      "       grad_fn=<NllLossBackward>)\n",
      "tensor([0.5082, 0.8022, 1.1371, 1.1705, 0.4464, 1.1886, 0.8744, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<IndexPutBackward>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle as pkl\n",
    "from collections import defaultdict,deque,Counter,OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from models import LM_latent\n",
    "from vocab import Vocabulary\n",
    "\n",
    "N = 10\n",
    "criterion = nn.CrossEntropyLoss(reduction='none', ignore_index=-1)\n",
    "groundtruth = torch.rand(N, ).ge(0.5).type(torch.LongTensor)\n",
    "groundtruth[7:] = -1\n",
    "pred = torch.rand(N, 2, requires_grad=True)\n",
    "\n",
    "print(\"pred: \", pred)\n",
    "print(\"gtruth: \", groundtruth)\n",
    "\n",
    "loss = criterion(pred, groundtruth)\n",
    "print(loss)\n",
    "#print(pred.grad)\n",
    "\n",
    "# Manual approach\n",
    "#pred.grad.zero_()\n",
    "target = groundtruth[groundtruth!=-1]\n",
    "output = pred[groundtruth!=-1]\n",
    "loss_manual = -1 * F.log_softmax(output, 1).gather(1, target.unsqueeze(1))\n",
    "#loss_manual = loss_manual.mean()\n",
    "#loss_manual.backward()\n",
    "print(loss_manual)\n",
    "#print(pred.grad)\n",
    "\n",
    "#third Manual approach\n",
    "#pred.grad.zero_()\n",
    "target_2 = groundtruth[groundtruth!=-1]\n",
    "output_2 = pred[groundtruth!=-1]\n",
    "criterion3 = nn.CrossEntropyLoss(reduction='none')\n",
    "loss_manual_3 = criterion3(output_2, target_2)\n",
    "#loss_manual_3.backward()\n",
    "print(loss_manual_3)\n",
    "#print(pred.grad)\n",
    "\n",
    "#another manual approach\n",
    "#pred.grad.zero_()\n",
    "criterion2 = nn.CrossEntropyLoss(reduction='none')\n",
    "loss2 = criterion2(pred, groundtruth)\n",
    "msk = (groundtruth == -1)\n",
    "loss2[msk] = 0\n",
    "# nonzeroitems = (~msk).sum().item()\n",
    "# loss2 = torch.sum(loss2)\n",
    "# loss2 = loss2/nonzeroitems\n",
    "# loss2.backward()\n",
    "print(loss2)\n",
    "# print(pred.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False, False,  True])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([4, 6,1, -5, 0])\n",
    "b = torch.Tensor([4,3,0,1,0])\n",
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3987, 0.9295, 0.9203, 0.8843, 0.8436, 0.6385, 0.6389, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "criterion = nn.CrossEntropyLoss(reduction='none', ignore_index=-1)\n",
    "groundtruth = torch.rand(N, ).ge(0.5).type(torch.LongTensor)\n",
    "groundtruth[7:] = -1\n",
    "pred = torch.rand(N, 2, requires_grad=True)\n",
    "\n",
    "loss = criterion(pred, groundtruth)\n",
    "print(loss)\n",
    "#print(pred.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
