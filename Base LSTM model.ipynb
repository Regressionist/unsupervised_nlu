{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle as pkl\n",
    "from collections import defaultdict,deque,Counter\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/rj1408/pos_lm/ptb_wsj_pos/train.p\",\"rb\") as f:\n",
    "    traindict = pkl.load(f)\n",
    "with open(\"/scratch/rj1408/pos_lm/ptb_wsj_pos/val.p\",\"rb\") as f:\n",
    "    valdict = pkl.load(f)\n",
    "with open(\"/scratch/rj1408/pos_lm/ptb_wsj_pos/test.p\",\"rb\") as f:\n",
    "    testdict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "974254"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindict['tagged_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter([word[0] if not word[0].isnumeric() else 'UNK' for word in traindict['tagged_words']])\n",
    "generic_vocab = ['SOS','EOS','PAD']+list([w for w in word_freq if word_freq[w]>5])\n",
    "generic_word2id = {}\n",
    "generic_id2word = {}\n",
    "for i,word in enumerate(generic_vocab):\n",
    "    generic_word2id[word] = i\n",
    "    generic_id2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_word2id = defaultdict(dict)\n",
    "tagged_id2word = defaultdict(dict)\n",
    "knowledgebase = defaultdict(deque)\n",
    "pos_sizes = defaultdict(int)\n",
    "for (word,tag) in traindict['tagged_words']:\n",
    "    if word not in tagged_word2id[tag]:\n",
    "        if word in word_freq:\n",
    "            tagged_word2id[tag][word] = pos_sizes[tag]+4\n",
    "            tagged_id2word[tag][pos_sizes[tag]+4] = word\n",
    "            knowledgebase[tag].append(word)\n",
    "            pos_sizes[tag]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in knowledgebase.keys():\n",
    "    tagged_word2id[tag]['SOS'] = 0\n",
    "    tagged_id2word[tag][0] = 'SOS'\n",
    "    tagged_word2id[tag]['EOS'] = 1\n",
    "    tagged_id2word[tag][1] = 'EOS'\n",
    "    tagged_word2id[tag]['PAD'] = 2\n",
    "    tagged_id2word[tag][2] = 'PAD'\n",
    "    tagged_word2id[tag]['UNK'] = 3\n",
    "    tagged_id2word[tag][3] = 'UNK'\n",
    "    knowledgebase[tag].appendleft('UNK')\n",
    "    knowledgebase[tag].appendleft('PAD')\n",
    "    knowledgebase[tag].appendleft('EOS')\n",
    "    knowledgebase[tag].appendleft('SOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id = defaultdict(int)\n",
    "id2tag = defaultdict(str)\n",
    "for i, tag in enumerate(tagged_word2id.keys()):\n",
    "    tag2id[tag] = i\n",
    "    id2tag[i] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 4, '?': 5, '!': 6, 'SOS': 0, 'EOS': 1, 'PAD': 2, 'UNK': 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_word2id['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBDataset(object):\n",
    "    def __init__(self, instanceDict, word2id, id2word):\n",
    "        self.root = instanceDict['tagged_sents']\n",
    "        self.id2word = id2word\n",
    "        self.word2id = word2id\n",
    "        self.sents = [[s[0] for s in sentences] for sentences in self.root]\n",
    "        self.sents.sort(key=lambda x:len(x))\n",
    "        #self.tags = [[s[1] for s in sentences] for sentences in self.root]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.root)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        target_sent = [self.word2id[word] if word in self.word2id else self.word2id['UNK'] for word in self.sents[idx]]\n",
    "        input_sent = [self.word2id['SOS']] + target_sent\n",
    "        target_sent.append(self.word2id['EOS'])\n",
    "        return (torch.as_tensor([input_sent], dtype=torch.long), torch.as_tensor([target_sent], dtype=torch.long))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindict['tagged_sents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDataset(object):\n",
    "    def __init__(self, instanceDict, word2id, tag2id):\n",
    "        self.root = instanceDict['tagged_sents']\n",
    "        self.tag2id = tag2id\n",
    "        self.word2id = word2id\n",
    "        self.root.sort(key=lambda x:len(x))\n",
    "        self.sents = [[s[0] for s in sentences] for sentences in self.root]\n",
    "        self.tags = [[s[1] for s in sentences] for sentences in self.root]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.root)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        target_labels = [self.tag2id[tag] for tag in self.tags[idx]]\n",
    "        input_sent = [self.word2id[word] if word in self.word2id else self.word2id['UNK'] for word in self.sents[idx]]\n",
    "        return (torch.as_tensor([input_sent], dtype=torch.long), torch.as_tensor([target_labels], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PTBDataset(traindict,generic_word2id,generic_id2word)\n",
    "val_dataset = POSDataset(valdict,generic_word2id,tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
    "    padded_list = []\n",
    "    for t in list_of_tensors:\n",
    "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
    "        padded_list.append(padded_tensor)\n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    return padded_tensor\n",
    "def pad_collate_fn_lm(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    input_list = [s[0] for s in batch]\n",
    "    target_list = [s[1] for s in batch]\n",
    "    pad_token = 2    \n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
    "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
    "    return input_tensor, target_tensor\n",
    "def pad_collate_fn_pos(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    input_list = [s[0] for s in batch]\n",
    "    target_list = [s[1] for s in batch]\n",
    "    pad_token_input = 2 \n",
    "    pad_token_tags = 46\n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_token_input)\n",
    "    target_tensor = pad_list_of_tensors(target_list, pad_token_tags)\n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=pad_collate_fn_lm, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=pad_collate_fn_pos, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self,vocab_size,hidden_size):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
