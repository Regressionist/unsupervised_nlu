{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle as pkl\n",
    "from collections import defaultdict,deque,Counter,OrderedDict\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.p\",\"rb\") as f:\n",
    "    traindict = pkl.load(f)\n",
    "with open(\"val.p\",\"rb\") as f:\n",
    "    valdict = pkl.load(f)\n",
    "with open(\"test.p\",\"rb\") as f:\n",
    "    testdict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tagset.txt') as f:\n",
    "    alltags = f.read()\n",
    "\n",
    "alltags = list(map(lambda strline: strline.split('\\t')[1], alltags.split('\\n')))\n",
    "alltags = set(alltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter([word[0] if not word[0].isnumeric() and word[1] in alltags else 'UNK' for word in traindict['tagged_words']])\n",
    "generic_vocab = ['SOS','EOS','PAD']+list([w for w in word_freq if word_freq[w]>5])\n",
    "generic_word2id = {}\n",
    "generic_id2word = {}\n",
    "for i,word in enumerate(generic_vocab):\n",
    "    generic_word2id[word] = i\n",
    "    generic_id2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledgebase = defaultdict(deque)\n",
    "for (word,tag) in traindict['tagged_words']:\n",
    "    if tag in alltags:\n",
    "        if word not in knowledgebase[tag]:\n",
    "            if word in word_freq and word_freq[word]>5:\n",
    "                knowledgebase[tag].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in knowledgebase.keys():\n",
    "    knowledgebase[tag].appendleft('UNK')\n",
    "    knowledgebase[tag].appendleft('PAD')\n",
    "    knowledgebase[tag].appendleft('EOS')\n",
    "    knowledgebase[tag].appendleft('SOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id = defaultdict(int)\n",
    "id2tag = defaultdict(str)\n",
    "for i, tag in enumerate(alltags):\n",
    "    tag2id[tag] = i\n",
    "    id2tag[i] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBDataset(object):\n",
    "    def __init__(self, instanceDict, word2id):\n",
    "        self.root = instanceDict['tagged_sents']\n",
    "        self.word2id = word2id\n",
    "        self.sents = [[s[0] for s in sentences] for sentences in self.root]\n",
    "        self.sents.sort(key=lambda x:len(x))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.root)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        target_sent = [self.word2id[word] if word in self.word2id else self.word2id['UNK'] for word in self.sents[idx]]\n",
    "        input_sent = [self.word2id['SOS']] + target_sent\n",
    "        target_sent.append(self.word2id['EOS'])\n",
    "        return (torch.as_tensor([input_sent], dtype=torch.long), torch.as_tensor([target_sent], dtype=torch.long))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDataset(object):\n",
    "    def __init__(self, instanceDict, word2id, tag2id):\n",
    "        self.root = instanceDict['tagged_sents']\n",
    "        self.tag2id = tag2id\n",
    "        self.word2id = word2id\n",
    "        self.root.sort(key=lambda x:len(x))\n",
    "        self.sents = [[s[0] for s in sentences] for sentences in self.root]\n",
    "        self.tags = [[s[1] for s in sentences] for sentences in self.root]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.root)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        target_labels = [self.tag2id[tag] for tag in self.tags[idx]]\n",
    "        target_sent = [self.word2id[word] if word in self.word2id else self.word2id['UNK'] for word in self.sents[idx]]\n",
    "        input_sent = [self.word2id['SOS']]+[self.word2id[word] if word in self.word2id else self.word2id['UNK'] for word in self.sents[idx]]\n",
    "        target_sent.append(self.word2id['EOS'])\n",
    "        return (torch.as_tensor([input_sent], dtype=torch.long), torch.as_tensor([target_sent], dtype=torch.long), torch.as_tensor([target_labels], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PTBDataset(traindict,generic_word2id)\n",
    "val_dataset = POSDataset(valdict,generic_word2id,tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
    "    padded_list = []\n",
    "    for t in list_of_tensors:\n",
    "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
    "        padded_list.append(padded_tensor)\n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    return padded_tensor\n",
    "def pad_collate_fn_lm(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    input_list = [s[0] for s in batch]\n",
    "    target_list = [s[1] for s in batch]\n",
    "    pad_token = 2    \n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
    "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
    "    return input_tensor, target_tensor\n",
    "def pad_collate_fn_pos(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    input_list = [s[0] for s in batch]\n",
    "    target_list = [s[1] for s in batch]\n",
    "    target_labels = [s[2] for s in batch]\n",
    "    pad_token_input = 2 \n",
    "    pad_token_tags = 37\n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_token_input)\n",
    "    target_tensor = pad_list_of_tensors(target_list, pad_token_input)\n",
    "    target_labels = pad_list_of_tensors(target_labels, pad_token_tags)\n",
    "    return input_tensor, target_tensor, target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38219"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=pad_collate_fn_lm, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=pad_collate_fn_pos, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self,vocab_size,hidden_size,token_embedding_size,tag_embedding_size,tags,mask,device):\n",
    "\n",
    "        super(LM, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.token_embedding_size = token_embedding_size\n",
    "        self.tag_embedding_size = tag_embedding_size\n",
    "        self.num_tags = len(tags)\n",
    "        self.tags = tags\n",
    "        self.mask = mask\n",
    "        self.device=device\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(self.vocab_size,self.token_embedding_size)\n",
    "        self.lstm = nn.LSTM(self.token_embedding_size, self.hidden_size, num_layers = 1, batch_first = True)\n",
    "        self.tag_linear = nn.Linear(self.tag_embedding_size,self.num_tags)\n",
    "        self.lower_hidden = nn.Linear(self.hidden_size,self.tag_embedding_size)\n",
    "        self.tag_projections = nn.ModuleList([nn.Linear(self.hidden_size,self.vocab_size) for tag in self.tags])\n",
    "        \n",
    "        \n",
    "    def forward(self,input_seq):\n",
    "        if self.training:\n",
    "            batch_size,sent_len = input_seq.shape[0],input_seq.shape[1]\n",
    "            h = torch.zeros((1,batch_size,self.hidden_size),device=self.device)\n",
    "            c = torch.zeros((1,batch_size,self.hidden_size),device=self.device)\n",
    "            embeddings = self.token_embedding(input_seq) #batch_size, sent_len, embed_size\n",
    "            outputs = torch.zeros((batch_size,sent_len,self.vocab_size),device=device)\n",
    "            for idx in range(sent_len):\n",
    "                embedding_input = embeddings[:,idx,:].view(batch_size,1,self.token_embedding_size)\n",
    "                _,(h,c) = self.lstm(embedding_input,(h,c))\n",
    "                h_lower = self.lower_hidden(h.transpose(0,1).view(batch_size,-1)) #batch_size,100\n",
    "                tag_weights = F.softmax(self.tag_linear(h_lower),dim=-1) #batch_size,num_tags\n",
    "                word_distributions = torch.cat([(F.softmax(self.tag_projections[i](h.squeeze(0)),dim=-1)*self.mask[i]).unsqueeze(1) for i in range(self.num_tags)],dim=1)#batch_size,num_tags,vocab_size\n",
    "                attended_words = torch.bmm(tag_weights.unsqueeze(1),word_distributions)\n",
    "                outputs[:,idx,:] = attended_words.squeeze(1)\n",
    "            return torch.log(outputs)\n",
    "        elif self.eval:\n",
    "            with torch.no_grad():\n",
    "                batch_size,sent_len = input_seq.shape[0],input_seq.shape[1]\n",
    "                h = torch.zeros((1,input_seq.shape[0],self.hidden_size),device=self.device)\n",
    "                c = torch.zeros((1,input_seq.shape[0],self.hidden_size),device=self.device)\n",
    "                embeddings = self.token_embedding(input_seq) #batch_size, sent_len, embed_size\n",
    "                pred_tag = torch.zeros((batch_size,sent_len),device=device)\n",
    "                pred_word = torch.zeros((batch_size,sent_len,self.vocab_size),device=device)\n",
    "                for idx in range(sent_len):\n",
    "                    embedding_input = embeddings[:,idx,:].view(batch_size,1,self.token_embedding_size)\n",
    "                    _,(h,c) = self.lstm(embedding_input,(h,c))\n",
    "                    h_lower = self.lower_hidden(h.transpose(0,1).view(batch_size,-1)) #batch_size,100\n",
    "                    tag_weights = F.softmax(self.tag_linear(h_lower),dim=-1) #batch_size,num_tags\n",
    "                    pred_tag[:,idx] = torch.argmax(tag_weights,dim=-1) \n",
    "                    \n",
    "                    word_distributions = torch.cat([(F.softmax(self.tag_projections[i](h.squeeze(0)),dim=-1)*self.mask[i]).unsqueeze(1) for i in range(self.num_tags)],dim=1)#batch_size,tag_vocab_size\n",
    "                    attended_words = torch.bmm(tag_weights.unsqueeze(1),word_distributions)\n",
    "                    pred_word[:,idx,:] = attended_words.squeeze(1)\n",
    "            \n",
    "                return pred_tag,torch.log(pred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(generic_vocab)\n",
    "HIDDEN_SIZE = 512\n",
    "EMBEDDING_SIZE = 256\n",
    "TAG_EMBEDDING_SIZE = 128\n",
    "device = torch.device('cuda:0')\n",
    "mask = torch.zeros(len(knowledgebase.keys()),VOCAB_SIZE,device=device)\n",
    "for i,tag in enumerate(knowledgebase.keys()):\n",
    "    idx = [generic_word2id[word] for word in knowledgebase[tag]]\n",
    "    mask[i,idx] = 1\n",
    "lang_model = LM(vocab_size=VOCAB_SIZE, \n",
    "                hidden_size=HIDDEN_SIZE,\n",
    "                token_embedding_size=EMBEDDING_SIZE,\n",
    "                tag_embedding_size=TAG_EMBEDDING_SIZE,\n",
    "                tags = knowledgebase.keys(), \n",
    "                mask = mask,\n",
    "               device = device).to(device)\n",
    "criterion = nn.NLLLoss(ignore_index=2)\n",
    "optimizer = optim.Adam(lang_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,val_loader,criterion,device):\n",
    "    model.eval()\n",
    "    token_acc = 0\n",
    "    total_tokens = 0\n",
    "    sent_acc = 0\n",
    "    total_sent = 0\n",
    "    val_nll = 0\n",
    "    for batch,(input_seq,target_seq,target_labels) in enumerate(val_loader):\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_labels = target_labels.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "        pred_labels,pred_words = model(input_seq)\n",
    "        match = pred_labels[:,:-1].long()==target_labels\n",
    "        token_acc+=float(match.sum())\n",
    "        total_tokens+= float(match.numel())\n",
    "        sent_acc+=float((match.sum(dim=-1)==match.shape[1]).sum())\n",
    "        total_sent+=float(match.shape[0])\n",
    "        loss = 0\n",
    "        for i in range(input_seq.shape[1]):\n",
    "            loss+= criterion(pred_words[:,i,:],target_seq[:,i])\n",
    "        loss/=(input_seq.shape[1]*input_seq.shape[0])\n",
    "        val_nll+=loss.item()\n",
    "    token_acc/=total_tokens\n",
    "    sent_acc/=total_sent\n",
    "    return token_acc,sent_acc,val_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,val_loader,optimizer,criterion,device,num_epochs=10):\n",
    "    c_point = 238*2\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for batch,(input_seq,target_seq) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred_seq = model(input_seq)\n",
    "            loss = 0\n",
    "            for i in range(input_seq.shape[1]):\n",
    "                loss+= criterion(pred_seq[:,i,:],target_seq[:,i])\n",
    "            loss/=(input_seq.shape[1]*input_seq.shape[0])\n",
    "            train_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (batch+1)%c_point==0:\n",
    "                token_acc,sent_acc, val_loss = evaluate(model,val_loader,criterion,device)\n",
    "                print('epoch: {} | step: {}/{} | train loss: {} | token acc: {} | sent acc: {} | val loss: {}'.format(epoch+1,\n",
    "                                                                                                                      (batch+1)//c_point,\n",
    "                                                                                                                      len(train_loader)//c_point,\n",
    "                                                                                                                      round(train_loss,3),\n",
    "                                                                                                                      round(token_acc,3),\n",
    "                                                                                                                      round(sent_acc,3),\n",
    "                                                                                                                      round(val_loss,3)))\n",
    "                train_loss = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | step: 1/5 | train loss: 156.628 | token acc: 0.149 | sent acc: 0.0 | val loss: 167.987\n",
      "epoch: 1 | step: 2/5 | train loss: 143.087 | token acc: 0.09 | sent acc: 0.0 | val loss: 123.553\n",
      "epoch: 1 | step: 3/5 | train loss: 138.746 | token acc: 0.05 | sent acc: 0.0 | val loss: 113.392\n",
      "epoch: 1 | step: 4/5 | train loss: 134.224 | token acc: 0.03 | sent acc: 0.0 | val loss: 103.509\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 22.38 GiB total capacity; 7.92 GiB already allocated; 21.06 MiB free; 721.22 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-682f54183511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-da2b689a0c07>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m/=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mc_point\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ashwin/miniconda3/envs/dl20/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ashwin/miniconda3/envs/dl20/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 22.38 GiB total capacity; 7.92 GiB already allocated; 21.06 MiB free; 721.22 MiB cached)"
     ]
    }
   ],
   "source": [
    "train(lang_model,train_loader,val_loader,optimizer,criterion,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
